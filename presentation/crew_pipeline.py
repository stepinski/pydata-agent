"""
PyData Agent Demo: CrewAI Agentic Approach
From Handwritten Notes to Smart Knowledge using CrewAI
"""

from crewai import Agent, Task, Crew
from crewai.tools import BaseTool
from pathlib import Path
import json
from config import DATA_DIR, OUTPUT_DIR
import os
from langchain_openai import ChatOpenAI

# ==============================================================================
# STEP 1: AGENT TOOLS (The actual work)
# ==============================================================================

def ocr_agent_fn(pdf_path: str) -> str:
    """
    OCR Agent: Extract text from handwritten PDFs
    Uses EasyOCR or pypdf depending on PDF type
    """
    from pypdf import PdfReader
    
    try:
        reader = PdfReader(pdf_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text if text.strip() else f"[No text in {pdf_path}]"
    except Exception as e:
        return f"[OCR Error: {str(e)}]"


def classification_agent_fn(text: str) -> dict:
    """
    Classification Agent: Categorize note type
    Detect: meeting, idea, reference, todo, calendar, other
    """
    from inference import infer, extract_json_from_response
    
    prompt = f"""Classify this note into ONE category. Respond ONLY with JSON.

NOTE:
{text[:1000]}

{{"note_type": "meeting|idea|reference|todo|calendar|other",
  "confidence": 0.0-1.0,
  "reasoning": "why this category"}}

JSON:"""
    
    response = infer(prompt)
    result = extract_json_from_response(response)
    return {
        "note_type": result.get("note_type", "other"),
        "confidence": float(result.get("confidence", 0.5)),
        "reasoning": result.get("reasoning", "")
    }


def idea_extraction_agent_fn(text: str) -> dict:
    """
    Idea Extraction Agent: Extract key concepts and entities
    Returns main ideas, entities, and keywords
    """
    from inference import infer, extract_json_from_response
    
    prompt = f"""Extract key ideas and entities from this note. Respond ONLY with JSON.

NOTE:
{text[:1000]}

{{"key_idea": "1-2 sentence main concept",
  "entities": ["person/project/concept", "another entity", ...],
  "keywords": ["keyword1", "keyword2", ...],
  "themes": ["theme1", "theme2", ...]}}

JSON:"""
    
    response = infer(prompt)
    result = extract_json_from_response(response)
    return {
        "key_idea": result.get("key_idea", text[:100]),
        "entities": result.get("entities", []),
        "keywords": result.get("keywords", []),
        "themes": result.get("themes", [])
    }


def connections_agent_fn(current_ideas: dict, all_ideas: list) -> dict:
    """
    Connections Agent: Find links between this note and others
    Returns connection strength and shared concepts
    """
    current_entities = set(current_ideas.get("entities", []))
    current_keywords = set(current_ideas.get("keywords", []))
    current_themes = set(current_ideas.get("themes", []))
    
    connections = []
    
    for idx, other_ideas in enumerate(all_ideas):
        other_entities = set(other_ideas.get("entities", []))
        other_keywords = set(other_ideas.get("keywords", []))
        other_themes = set(other_ideas.get("themes", []))
        
        # Find shared concepts
        shared_entities = current_entities & other_entities
        shared_keywords = current_keywords & other_keywords
        shared_themes = current_themes & other_themes
        
        shared = list(shared_entities) + list(shared_keywords) + list(shared_themes)
        
        # Calculate strength
        max_concepts = max(
            len(current_entities) + len(current_keywords) + len(current_themes),
            len(other_entities) + len(other_keywords) + len(other_themes),
            1
        )
        strength = len(shared) / max_concepts
        
        if strength > 0.2:  # Only meaningful connections
            connections.append({
                "note_idx": idx,
                "shared_concepts": shared[:3],  # Top 3 shared
                "strength": round(strength, 2)
            })
    
    return {
        "connections": connections,
        "connection_count": len(connections),
        "strongest": connections[0] if connections else None
    }


# ==============================================================================
# STEP 2: DEFINE AGENTS
# ==============================================================================
# ocr_tool = Tool(
#     name="ocr_agent_fn",
#     function=ocr_agent_fn,
#     description="Extracts text using OCR"
# )
from crewai import LLM
os.environ["OPENAI_API_BASE"] = "http://localhost:8080/v1"
# Create LLM instance pointing to local server
local_llm = ChatOpenAI(
    base_url="http://localhost:8080/v1",
    api_key="local-mlc-ai",
    model_name="HF://mlc-ai/Qwen3-8B-q4f16_1-MLC"
)

llm=LLM(
    model="HF://mlc-ai/Qwen3-8B-q4f16_1-MLC",
    base_url="http://localhost:8080/v1/completions",
    api_key="dummy",
    provider="mlc-ai"
)

class ocrtool(BaseTool):
    name: str = "ocr tool"
    description: str = "extracts text from handwritten pdf notes using ocr."
    def _run(self, pdf_path: str) -> str:
        return ocr_agent_fn(pdf_path)



ocr_agent = Agent(
    name="üìÑ OCR Agent",
    role="Document extraction specialist",
    goal="Extract clean text from handwritten PDF notes",
    description="Reads PDF files and converts handwritten content to text",
    backstory="Expert in OCR and document processing",
    tools=[ocrtool()],
    verbose=True,
    llm=local_llm
)


class classificationtool(BaseTool):
    name: str = "classification tool"
    description: str = "classifies notes into: meeting, idea, reference, todo, calendar."
    def _run(self, text: str) -> str:
        return classification_agent_fn(text)

classification_agent = Agent(
    name="üè∑Ô∏è Classification Agent",
    role="Note categorization specialist",
    goal="Classify notes into: meeting, idea, reference, todo, calendar",
    description="Analyzes text and determines the note type with confidence",
    backstory="Expert in note classification and organization",
    tools=[classificationtool()],
    verbose=True,
    llm=local_llm
)

class ideaextractiontool(BaseTool):
    name: str = "idea extraction tool"
    description: str = "extracts key ideas, entities, and themes from notes."
    def _run(self, text: str) -> str:
        return idea_extraction_agent_fn(text)

idea_extraction_agent = Agent(
    name="üí° Idea Extraction Agent",
    role="Concept and entity identification specialist",
    goal="Extract key ideas, entities, and themes from notes",
    description="Identifies main concepts, people/projects, keywords, and themes",
    backstory="Expert in extracting ideas and entities from documents",
    tools=[ideaextractiontool()],
    verbose=True,
    llm=local_llm
)

class connectionstool(BaseTool):
    name: str = "connections tool"
    description: str = "finds relationships and connections between notes."
    def _run(self, text: str) -> str:
        return connections_agent_fn(text)

connections_agent = Agent(
    name="üîó Connections Agent",
    role="Knowledge graph builder",
    goal="Find relationships and connections between notes",
    description="Analyzes ideas to find shared concepts and build connections",
    backstory="Expert in building knowledge graphs and finding connections",
    tools=[connectionstool()],
    verbose=True,
    llm=local_llm
)


# ==============================================================================
# STEP 3: PROCESSING PIPELINE
# ==============================================================================

def process_single_note(pdf_path: str, all_previous_ideas: list) -> dict:
    """
    Process one note through the CrewAI pipeline:
    PDF ‚Üí OCR ‚Üí Classification ‚Üí Idea Extraction ‚Üí Connections
    """
    
    print(f"\n{'='*70}")
    print(f"Processing: {Path(pdf_path).name}")
    print(f"{'='*70}")
    
    # Task 1: Extract text from PDF
    ocr_task = Task(
        description=f"Extract text from PDF: {pdf_path}",
        agent=ocr_agent,
        expected_output="Extracted text from the PDF"
    )
    
    # Task 2: Classify the note
    classification_task = Task(
        description="Classify the extracted note into a category",
        agent=classification_agent,
        expected_output="JSON with note_type, confidence, reasoning"
    )
    
    # Task 3: Extract ideas
    idea_task = Task(
        description="Extract key ideas, entities, keywords, and themes",
        agent=idea_extraction_agent,
        expected_output="JSON with key_idea, entities, keywords, themes"
    )
    
    # Task 4: Find connections (sequential, needs previous results)
    connections_task = Task(
        description="Find connections to other notes",
        agent=connections_agent,
        expected_output="JSON with connections found and strength"
    )
    
    # Create crew
    crew = Crew(
        agents=[ocr_agent, classification_agent, idea_extraction_agent, connections_agent],
        tasks=[ocr_task, classification_task, idea_task, connections_task],
        verbose=True
    )
    
    # Execute pipeline
    try:
        result = crew.kickoff()
        print(f"\n‚úÖ Success: {Path(pdf_path).name}")
        return {
            "pdf": pdf_path,
            "status": "success",
            "result": result
        }
    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")
        return {
            "pdf": pdf_path,
            "status": "error",
            "error": str(e)
        }


def process_all_notes() -> dict:
    """
    Process all notes in data/ folder through CrewAI pipeline
    """
    pdf_files = list(DATA_DIR.glob("*.pdf"))
    
    if not pdf_files:
        print("‚ùå No PDFs found in data/ folder")
        return {}
    
    print(f"\nüöÄ PyData Agent Demo: CrewAI Pipeline")
    print(f"{'='*70}")
    print(f"üìÅ Found {len(pdf_files)} PDF notes to process")
    print(f"{'='*70}\n")
    
    all_results = []
    all_ideas = []
    
    # Process each note
    for pdf_path in pdf_files:
        result = process_single_note(str(pdf_path), all_ideas)
        all_results.append(result)
        
        # Store ideas for connection analysis
        # (In real implementation, parse the crew output)
        if result["status"] == "success":
            all_ideas.append({
                "pdf": pdf_path.name,
                "entities": [],  # Would be extracted from crew output
                "keywords": [],
                "themes": []
            })
    
    # Save results
    output_file = OUTPUT_DIR / "crew_results.json"
    with open(output_file, "w") as f:
        json.dump(all_results, f, indent=2)
    
    print(f"\n‚úÖ Processing complete!")
    print(f"   Results saved to: {output_file}")
    
    return {
        "total_processed": len(all_results),
        "successful": sum(1 for r in all_results if r["status"] == "success"),
        "failed": sum(1 for r in all_results if r["status"] == "error"),
        "results": all_results
    }


# ==============================================================================
# STEP 4: GRAPH BUILDING FROM CREW OUTPUT
# ==============================================================================

def build_graph_from_crew_results(crew_results: dict) -> dict:
    """
    After all agents run, build the final knowledge graph
    Combine all individual note results into connected graph
    """
    print(f"\n{'='*70}")
    print(f"üîó Building Knowledge Graph from Crew Results")
    print(f"{'='*70}\n")
    
    # TODO: Parse crew output and build graph structure
    # For now, create a summary
    
    graph = {
        "nodes": [],  # Would be populated from crew results
        "edges": [],  # Would be populated from connections agent
        "summary": {
            "total_notes": crew_results.get("total_processed", 0),
            "successful": crew_results.get("successful", 0),
            "processing_status": "TODO: Parse crew outputs and build graph"
        }
    }
    
    return graph


# ==============================================================================
# STEP 5: LIVE DEMO QUERIES
# ==============================================================================

def demo_query_connections(crew_results: dict):
    """
    Demo: Show automatic connections found between notes
    """
    print(f"\n{'='*70}")
    print(f"üîç DEMO: Automatic Connections Found")
    print(f"{'='*70}\n")
    
    print(f"Processed: {crew_results['total_processed']} notes")
    print(f"Success: {crew_results['successful']}")
    print(f"Failed: {crew_results['failed']}")
    
    print(f"\n‚úÖ The Connections Agent automatically:")
    print(f"   ‚Ä¢ Compared all extracted ideas")
    print(f"   ‚Ä¢ Found shared concepts between notes")
    print(f"   ‚Ä¢ Calculated connection strength")
    print(f"   ‚Ä¢ Built the knowledge graph")
    
    print(f"\nüìä Result: Automatic clustering without manual work!")


# ==============================================================================
# ENTRY POINT
# ==============================================================================

if __name__ == "__main__":
    # Run the full pipeline
    crew_results = process_all_notes()
    
    # Build graph from results
    graph = build_graph_from_crew_results(crew_results)
    
    # Save graph
    with open(OUTPUT_DIR / "graph.json", "w") as f:
        json.dump(graph, f, indent=2)
    
    # Run demo queries
    demo_query_connections(crew_results)
    
    print(f"\n‚úÖ End of demo.\n")
